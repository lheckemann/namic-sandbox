
%\documentclass [12pt]{amsart}
%\documentclass [12pt,twocolumn]{amsbook}
\documentclass [10pt,twocolumn,twoside,final,letterpaper]{report}
\usepackage {amssymb}
\usepackage {amsmath}
\usepackage {graphicx}
\usepackage {rotating}
\usepackage {supertabular}
%\usepackage {multicol}
\usepackage {makeidx}
\usepackage [hyperref,colorlinks]{hyperref}
\usepackage{authordate1-4}        %Alternate citation format
\makeindex
%%The following make better use of the paper, but may not be as readable
\pagestyle{headings}
%\href{http://www-h.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html}{http://www-h.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html}
%%* Page Layout
%      \columnsep
%      % gap between columns
%      \topmargin -0.5in
%      % gap above header
%      \topskip
%      % between header and text
%      \textheight 8.5in
%      % height of main text
%      \textwidth 7.0in
%      % width of text
%%      \oddsidemargin 0.0in
%      % odd page left margin
%%      \evensidemargin 0.0in
%      % even page left margin
%%* Paragraphs
%      \parindent
%      % indentation of paragraphs
%      \parskip
%      % gap between paragraphs
%%* Floats (tables and figures)
%      \floatsep \baselineskip
%      % space left between floats.
%      \textfloatsep \baselineskip
%      % space between last top float or first bottom float and the text.
%      \intextsep \baselineskip
%      % space left on top and bottom of an in-text float.
%      \dbltextfloatsep \baselineskip
%      % is \textfloatsep for 2 column output.
%      \dblfloatsep \baselineskip
%      % is \floatsep for 2 column output.
%      \abovecaptionskip 0.5\baselineskip
%      % space above caption
%      \belowcaptionskip 0.5\baselineskip
%      % space below caption
%%* Maths
%      \abovedisplayskip
%      % space before maths
%      \belowdisplayskip
%      % space after maths
%      \arraycolsep
%      % gap between columns of an array
%%* Lists
%      \topsep
%      % space between first item and preceding paragraph.
%      \partopsep
%      % extra space added to \topsep when environment starts a new paragraph.
%      \itemsep
%      % space between successive items.
%\def\baselinestretch{1.5}

\newcommand{\bcode}{\texttt}
\newcommand{\bbigcommand}[1]{\texttt{\large{\framebox{#1}}}}
\newcommand{\brainstwoprog}{\bcode{BRAINS2}}
\newcommand{\brainsprog}{\bcode{BRAINS}}
\newcommand{\miregprog}{\bcode{MattesMutualRegistration}}
\newcommand{\smallimagescale}{0.66}

\begin{document}


\title{ {
    Mutual Information Rigid Registrations\\
    of Whole-Brain 3D Images,\\
    Making Use of the Insight Toolkit}
    \thanks{Supported by NIMH Grants:
MH31593, MH40856, and MHCRC43271.} \\
\vspace{0.5in}
}
\author{Greg Harris, Hans Johnson
\thanks{ The University of Iowa Carver College of Medicine,
Department of Psychiatry NeuroImaging Center,
200 Hawkins Drive, Iowa City, IA 52242 }
}
\date{\today}
%\date{September 14, 2005}
\maketitle

\onecolumn
\begin{abstract}
The University of Iowa's Psychiatric Iowa Neuroimaging 
Consortium (PINC)
has developed a program for mutual information registration 
of \brainstwoprog{} \cite{magnotta:brains2} data
using \bcode{ITK} \cite{ibanez:ITKSoftwareGuide14} classes, 
called \miregprog{}.
\vspace{0.25in}\par
We have written a helper class for use with \bcode{ITK} 
called \bcode{itkMultiModal3DMutualRegistrationHelper}.
We are also providing a transform meeting the \bcode{ITK} standard
called \bcode{itkScaleVersor3DTransform}.
There is also an affine transform conversion class
called \bcode{CrossOverAffineSystem} that we use to convert
between representation standards, including a routine to 
orthonormalize scale-varying affine transforms when assigning
them to rigid affine transforms.
\vspace{0.25in}\par
We have navigated the maze of transform classes and registration
methods, and have emerged with useful advice about best practices
for registering 3D rigid multimodal MRI of the human brain.
We can report a logical order in which to test multi-modal image registration.
\end{abstract}

\chapter*{Acknowledgments} \index{acknowledgments}
\label{sec:acknowledgementsknow}

The following people should be recognized for
their contributions: Vincent A. Magnotta, Norman Kent Williams, 
Hans J. Johnson, Gregory Harris.
\vspace{0.25in}\par
The \brainstwoprog{} software was developed with the leadership of
Nancy C. Andreasen, M.D., Ph.D.
This work is supported in part by NIMH Grants\index{Grants, contributing}:
MH31593, MH40856, and MHCRC43271.

\begin{figure}[b]
    \large{
Disclaimer: The University of Iowa and the Psychiatric Iowa Neuroimaging 
Consortium (PINC) make no
claims and guarantees about the \miregprog{} software package. The software
package known herein as \miregprog{} should be used for research purposes only.
    }
\end{figure}
\vspace{2.25in}\par
Copyright {\copyright} 2007 Hans Johnson and Nancy Andreasen. All Rights
Reserved.
%\twocolumn


\tableofcontents
%\listoffigures
%\listoftables


\chapter{Overview of Best Practices} \label{sec:introduction}
We have developed a program for mutual information registration 
of \brainstwoprog{} \cite{magnotta:brains2} data
using \bcode{ITK} \cite{ibanez:ITKSoftwareGuide14} classes.
Our program, \miregprog{}, was modified from
\begin{verbatim}
Insight/Examples/Registration/ImageRegistration8.cxx,
\end{verbatim}
which is noteworthy as containing the only proven, satisfactory way
to do multi-modal 3D rigid image registration that is provided with \bcode{ITK}.
\bcode{ImageRegistration8} is in the \bcode{Examples} directory, 
and also sec. 8.5.3 in the \bcode{ITK} manual.

\vspace{0.25in}\par

We wish to report reduction to practice of the \bcode{ITK} Mattes mutual information
image registration method in our \brainstwoprog{} setting.
Considerations of how to get what we want from \brainstwoprog{} have dictated that we:
\begin{itemize}
\item fix the origin of the 3D voxel array at $(0,0,0)$;  
\item read and write transforms in the \brainstwoprog{} 
\bcode{.xfrm} file format that holds a single affine matrix in voxel-addressing
coordinates;  
\item copy and modify the \bcode{itkScaleSkewVersor3DTransform}
to get a version with $3$ dimensions of scale and no skew aspect;
\item implement a \bcode{vnl\_svd}-based $3 \times{} 3$ matrix orthonormalization
routine and use it when coercing our \bcode{itkScaleVersor3DTransform} to an
\bcode{itkVersorRigid3DTransform}.
\end{itemize}

By closely following \bcode{ITK}'s Image Registration Example 8,
we have uncovered two pieces of necessary advice:
\begin{itemize}
\item initialize the transform with image centers of mass, which
is \emph{required} if the mutual information registration method is to work at all well;
\item avoid the pyramid of successive fits when attempting
a non-high-dimensional warping application, since the key in mutual information
registration is to use very large samples ($100000$ to $1000000$ voxels) drawn from
the images being registered.
\end{itemize}
Our application also permits us to limit attention
of the fit to a region of interest by loading a \brainstwoprog{} mask as an \bcode{ITK} boolean image.

This paper will be of limited interest to people who don't 
particularly care about analyzing brain imaging data 
in collaboration with the Iowa brain imaging initiative.
Those who do will be interested in how we can now provide separate executables
to do work on \brainstwoprog{} data, and our templated 
class \bcode{CrossOverAffineSystem} that converts
back and forth between \bcode{ITK} affine transforms and \brainstwoprog{} affine transforms.

\vspace{0.25in}\par

The method is not just statistically reliable;  we can and do depend on it for fast, 
exacting, multi-modal rigid brain image registration.  For example, we believe
this method is the way we should register later scans of the same human subject
to an earlier scan in a longitudinal study.  For another example, we can use this
method to fit acquisition scans to an average brain template 
with standardized Talairach atlas orientation.\index{phantom, digital}

\section{Our over-all approach to rigid 3D registration using \bcode{ITK}}

In one line, the \bcode{ITK} Manual decrees\index{mutual information}
\begin{quotation}
It has been extensively shown that metrics based on the evaluation
of mutual information is the best way to overcome the difficulties
of multi-modality registration. (sec.8.4.1)
\end{quotation}
\par
Of the two mutual information metrics in \bcode{ITK}, the
\bcode{itkMattesMutualInformationImageToImageMetric} is asymptotically
preferable in that with the Mattes metric, you can afford to sample $n=100000$ or more
voxels when computing a statistical transform parameter gradient in a second or two,
compared to maybe a 500 voxel sample in the same time with the earlier Viola-Wells 
mutual information metric that has proven serviceable in 2D imaging.
For 3D imaging with enough detail to depict the human brain,
we cannot afford a metric that runs inn $O(n^2)$ time .

There is also asymptotic advantage to using the \bcode{itkVersorRigid3DTransform}
for gradient descent, since a 6-dimensional parameter space is one order of
magnitude easier to traverse than a 7-dimensional parameter space 
for the \bcode{QuaternionRigidTransform}.

Even though a lot of attention is given to using a multi-resolution image
pyramid for registration in sec. 8.6 of the \bcode{ITK} Manual, the successful
multi-modal method does not initialize the transform refinement
from scratch with a very small pyramid apex.
Instead, you \emph{must} initialize using \bcode{itkCenteredTransformInitializer},
which chooses a rotation \bcode{Center} for subsequent optimization based on 
the fixed and moving input images.
\bcode{itkCenteredTransformInitializer} uses signal centers of gravity on the assumption that
the background signal intensity always vanishes to zero outside the subject's head.
If the \bcode{Center} of a transform with rotation is not a translation mapping 
figure-middle in one image to figure-middle in the other,
any correction to rotation parameters will badly spoil 
cumulative translation parameter correctness.

It is also important to respect \bcode{ImageRegistration8}'s chronologically linear,
lock-step, in-place use of a single working transform object through the course
of a data-run.  If you throw the transform away and re-construct
it from numeric parameters alone on the other side of a module boundary 
(say, in a \bcode{SimpleApp} solution),
you will accidentally lose the transform Center, which is
implied in neither the Matrix nor the Offset aspects of a transform.

\section{A helper class to build an ITK pipeline}
We propose a helper class we developed, \bcode{itkMultiModal3DMutualRegistrationHelper},
for possible inclusion in or use with \bcode{ITK}.  The helper class abstracts an interface boundary,
within which all is according to \bcode{ITK} and the \bcode{ImageRegistration8}
example, and outside which you may address the input/output of your actual data.
The \miregprog{} program is such a program for the \brainstwoprog{} world.

This means most of the code corresponding to \bcode{ImageRegistration8.cxx}
will be found in the helper \bcode{itkMultiModal3DMutualRegistrationHelper.txx}.

\section{A helper class to convert between affine transform representations}
There is also an affine transform conversion class\index{transform conversion}
called \bcode{CrossOverAffineSystem} that we use to convert
between the two affine transform representation standards we work with,
Euclidean space in millimeters and voxel array addressing.
At one point, we thought \bcode{CrossOverAffineSystem} was 
going to be the centerpiece of our published way to
interconvert between transform representation standards, but it no longer seems 
such a seminal piece of algebra.  Suffice it to say, you invert
\[B = h A g\] with the reversal \[A = h^{-1} B g^{-1}\] to make
converted output the inverse of converted input. Here, $h$ and $g$ are conversion
definition matrices that can be built up in enclosing fashion;
$A$ might be an inter-image transform in voxel addressing space, and $B$
might be the same inter-image transform in Euclidean millimeters space.
If the fixed image is anisotropic and the moving image is isotropic, $h$ and $g$ might look like
\begin{equation*}
 g = \left( \begin{array}{rrr}
            \frac{1}{2} & 0 & 0 \\
            0 & \frac{1}{2} & 0 \\
            0 & 0 & \frac{1}{2} \\
            \end{array} \right)
\end{equation*}
and
\begin{equation*}
 h = \left( \begin{array}{rrr}
            2 & 0 & 0 \\
            0 & 2 & 0 \\
            0 & 0 & 3 \\
            \end{array} \right)
\end{equation*}
Yes, that's a 3.  The numbers down the diagonals of $h$ and $g^{-1}$ are the 
voxel dimensions in the sense of anisotropy.

\section{THIS NEXT SECTION NEEDS TOTAL TEXT REPLACEMENT.}
\emph{Do we discuss our automated workup project at all?  Leave this out?}

\section{We find an approximating phantom to fit to for each subject.}
Since we are imaging human brains, it makes practical sense to 
relate acquisition scans to an average brain atlas template using
fits that compute a transform with $3$ parameters of scale, for separately
varying rostro-caudal, medial-lateral, and superior-inferior head shape.
If we let
\begin{quotation}
 $S =$ an affine transform with scale variability,
\end{quotation}
and
\begin{quotation}
 $R =$ the rigid rotation and translation obtained by orthonormalizing the $3 \times{} 3$ matrix in $S$,
\end{quotation}
then $S R^{-1}$ and its inverse, $R S^{-1}$, let us work in terms of 
the scale-only factor in acquisition-to-atlas fit $S$.  This is useful to us in putting a
scale-adjusted atlas phantom in a subject's coordinate space, and
subsequently fitting the acquisition to a scale-appropriate, correctly
aligned phantom.\index{phantom, digital}  When we find the fit $S$,
we benefit from being able to initialize a $9$-parameter scaling fit
with the results of initially attempting a preliminary $6$-parameter rigid fit.


\chapter{Crafting and Testing a Code Framework} \label{sec:coding}

The use of test data when validating the code for image registration
neatly partitions two separate aspects distributed through the code:
\begin{itemize}
\item the fine structure of information processing 
expressed as mathematics and subject to the question, "Are the values correct?" and
\item the larger structure of class and executable interfaces
for assigning status to information flows among modules.
\end{itemize}

\vspace{0.25in}\par

Intuitively, the choice of information processing code 
is confirmed \emph{after} we use the test data, 
while the choice of interfaces exhibiting the desired data flow discipline
is developed \emph{prior to} the additional code to test the program's correctness.
Before we can even design the testing scripts to exercise the method's requirements, 
we need most of the code/interface \emph{framework} already in place.
The business end --- the algorithmic content --- 
can be dropped in to populate the method
with content-dependent information processing 
\emph{after} we have confidence in our ability to
verify what such combinations are doing.

\section{Testing} \label{sec:testing}

Our verification of what computing was done recapitulates
the building and launching of \bcode{ITK} pipelines.
The helper class \bcode{itkMultiModal3DMutualRegistrationHelper}
creates and launches one such pipeline.
We programmed the ability to skip over the actual registration
so we could test transform input for optional initialization coupled to transform output.
\vspace{0.25in}\par
Actually, our system for using specific test data to methodically 
validate a rigid registration method based on image-image correspondence, 
is potentially of greater interest than 
that we happen to possess an implementation that 
cleared the hurdles of one such testing programme.
\vspace{0.25in}\par
Issues in testing\index{testing, issues in} include 
\begin{itemize}
\item voxel coordinate-based transforms as found in \bcode{AIR} \cite{web:AIR} 
and \brainstwoprog{} versus
\bcode{ITK}'s Euclidean space-based transforms; 
\item distinguishing coding errors in transform input-output from
incorrectly judging that the images are well-enough in correspondence when transformed;
\item voxel anisotropy and its potential impact on representation of rotation and scaling;
\item how a need to rotate for a solution can interfere with
previously demonstrated ability to find the translation-only solution;
\end{itemize}
\par
\section{Testing against a digital phantom} \label{sec:phantom}\index{phantom, digital}
We use our \brainstwoprog{} platform to generate\index{test data, generation of} isotropic and anisotropic
2 millimeter-scale test images from a real T1 MRI scan of an anonymous human subject.
\brainstwoprog{} can then provide image statistics for an image subtraction.
A mean near 0 and a low standard deviation numerically corresponds to a good fit.

We specified twisting and shifting transforms parametrically in \brainstwoprog{},
which we used to resample a twisted-and-shifted target image (the 'fixed' image).
This let us test mutual information registration by attempting to reconstruct 
the transform that registers the original image (the 'moving' image) to the transformed target.

\section{Confidence building tests} \label{sec:confidence}\index{tests, confidence building}
Debugging code into existence involves a logical order of what to attempt.
\begin{itemize}
\item transform file input-output idempotence, 
to check the representation and conversion of spatial transforms;
\item initialize the registration transform with its known solution, 
and see that the registration method so supplied converges readily;
\item let the registration method start from way off or with no transform initialization 
and see if it converges tractably and reliably;
\end{itemize}
Attempt these three steps for a translation-only test case,
and then for a translation and rotation test case.
You may as well begin with the anisotropic voxel image,
for starting with isotropic images can be misleading and is no simpler to sustain.


\chapter{Conclusion} \label{sec:conclusion}

\section{Results} \label{sec:results}
The \miregprog{} software is robustly convergent on the
kind of MRI/fMRI data we need to register in our brain imaging lab.
It is predictably convergent in under 200 gradient descent steps.
In fact, \emph{once you understand that you must separately establish
the transform Center},
an \bcode{ITK} 3D rigid image registration tool to maximize mutual information
can be considered reliable lab equipment.\index{lab equipment, scientific}

\section{Future work} \label{sec:futurework}
Although a download\index{download, url} of the \brainstwoprog{} source tree from
\begin{verbatim}
http://www.psychiatry.uiowa.edu/wiki/index.php?title=BRAINS_GETTING_STARTED
\end{verbatim}
will include \miregprog{} and its test data under
\begin{verbatim}
brains2/src/iplProg/MutualRegistration,
\end{verbatim}
--- see the \brainstwoprog{} script examples in
\begin{verbatim}
brains2/src/iplProg/MutualRegistration/PrincipalDebugTesting.tcl,
\end{verbatim}
we are considering whether to break out a smaller tarball version
of just \miregprog{} and test data in some form that can exemplify
and test the helper class \bcode{itkMultiModal3DMutualRegistrationHelper}
and the use it makes of other \bcode{ITK} classes,
as a kind of contribution to \bcode{InsightApplications}.

\appendix

\chapter{Usage} \label{sec:usage}
The command line usage of \miregprog{} is documented below.
Every parameter is to be given with a keyword followed by defining text,
like a number or a file name.  Defaults are in parentheses.
\vspace{0.25in}\par
Two parameters are required:
\vspace{0.25in}\par
\begin{tabular}{rp{0.25in}l}
\texttt{~~-f} && \parbox[t]{3.75in}{The name of the fixed image.\vspace{0.05in}}\\
\texttt{~~-m} && \parbox[t]{3.75in}{The name of the moving image.\vspace{0.05in}}\\
\end{tabular}
\vspace{0.25in}\par
The rest of the parameters are optional:
\vspace{0.25in}\par
Optional System tags:
\vspace{0.25in}\par
\begin{tabular}{rp{0.25in}l}
\texttt{-v,-h} && \parbox[t]{3.75in}{List options in short format}\\
\texttt{-V,-H} && \parbox[t]{3.75in}{List options in long format}\\
\texttt{-vxml,-hxml,-exportXML} && \parbox[t]{3.75in}{List options in xml format for BatchMake}\\
\texttt{-vgad,-hgad,-exportGAD} && \parbox[t]{3.75in}{List options in Grid Application Description format}\\
\texttt{-version} && \parbox[t]{3.75in}{return the version number}\\
\texttt{-date} && \parbox[t]{3.75in}{return the cvs checkout date}\\
 \end{tabular}
\par
\newpage
Optional Command tags (I): 
\vspace{0.25in}\par
\begin{tabular}{rp{0.25in}l}
\texttt{-cor <Flag>} && \parbox[t]{3.75in}{A boolean for putting both images in Coronal orientation.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Flag && (Default = 0)\\
\end{tabular}}\\
\texttt{-i <Filename>} && \parbox[t]{3.75in}{File name for an initial affine transformation, if desired.}\\
\texttt{-o <Filename>} && \parbox[t]{3.75in}{File name for the estimated transform to register the moving image to the fixed image.}\\
\texttt{-pid <IdString>} && \parbox[t]{3.75in}{A string to identify the research subject.  Formerly hospital patient id.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& IdString && (Default = unknown)\\
\end{tabular}}\\
\texttt{-qid <IdString>} && \parbox[t]{3.75in}{A string to identify the particular scanner encounter for this subject.  At Iowa, this is called the MRQID.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& IdString && (Default = unknown)\\
\end{tabular}}\\
\texttt{-no <Filename>} && \parbox[t]{3.75in}{File name for the estimated transform, stripped of scaling, to register the moving image to the fixed image.}\\
\texttt{-io <Flag>} && \parbox[t]{3.75in}{A boolean for testing transform representation idempotence (0).  Leave this alone, mostly.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Flag && (Default = 0)\\
\end{tabular}}\\
\texttt{-xfm <Code>} && \parbox[t]{3.75in}{Code:  A 0 means fit a 6-parameter rigid transform; a 1 means fit a 12-parameter affine transform; a 2 means fit a 9-parameter ScaleVersor3D transform; a 3 is no longer meaningful here; a 4 means fit a 15-parameter ScaleSkewVersor3D transform.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Code && (Default = 0)\\
\end{tabular}}\\
\texttt{-r <Filename>} && \parbox[t]{3.75in}{The name of the resampled output image, if desired.}\\
\texttt{-rt <Code } && \parbox[t]{3.75in}{The datatype of the resampled output image, if -r was requested (short).  Choose from float, short, ushort, int, uint, char, uchar.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Code && (Default = short)\\
\end{tabular}}\\
\texttt{-ft < Integer >} && \parbox[t]{3.75in}{The index in the time series for the 3D fixed image to fit, if 4-dimensional.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Integer && (Default = 0)\\
\end{tabular}}\\
\texttt{-mt <Integer>} && \parbox[t]{3.75in}{The index in the time series for the 3D moving image to fit, if 4-dimensional.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Integer && (Default = 0)\\
\end{tabular}}\\
\texttt{-fo <XLoc> <YLoc> <ZLoc>} && \parbox[t]{3.75in}{The co-ordinates of the AC point of the fixed image.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& XLoc && (Default = 0)\\
& YLoc && (Default = 0)\\
& ZLoc && (Default = 0)\\
\end{tabular}}\\
\texttt{-mo <XLoc> <YLoc> <ZLoc>} && \parbox[t]{3.75in}{The co-ordinates of the AC point of the fixed image.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& XLoc && (Default = 0)\\
& YLoc && (Default = 0)\\
& ZLoc && (Default = 0)\\
\end{tabular}}\\
\end{tabular}
\par
\newpage
Optional Command tags (II): 
\vspace{0.25in}\par
\begin{tabular}{rp{0.25in}l}
\texttt{-n < Integer >} && \parbox[t]{3.5in}{The number of iterations (0, which means unbounded).  Use an explicit limit like 500 or 1000 to manage risk of divergence while fitting in batch mode.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Integer && (Default = 0)\\
\end{tabular}}\\
\texttt{-s < Integer >} && \parbox[t]{3.5in}{The number of spatial samples (100000).  Increase this for a slower, more careful fit.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Integer && (Default = 100000)\\
\end{tabular}}\\
\texttt{-ts <Float>} && \parbox[t]{3.5in}{Translation scale compensation factor (1000.0).  Decrease this to put more rotation in the search pattern.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Float && (Default = 1000.0)\\
\end{tabular}}\\
\texttt{-rs <Float>} && \parbox[t]{3.5in}{ScaleVersor3D 'Scale' compensation factor (25.0).  Increase this to put more rescaling in a '-xfm 2' or '-xfm 4' search pattern.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Float && (Default = 25.0)\\
\end{tabular}}\\
\texttt{-ss <Float>} && \parbox[t]{3.5in}{ScaleSkewVersor3D 'Skew' compensation factor (10.0).  Increase this to put more skew in a '-xfm 4' search pattern.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Float && (Default = 10.0)\\
\end{tabular}}\\
\texttt{-l < Float >} && \parbox[t]{3.5in}{Relaxation factor (0.5).  Leave this alone.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Float && (Default = 0.5)\\
\end{tabular}}\\
\texttt{-max <Float>} && \parbox[t]{3.5in}{Maximum Step Length (0.2000).  Leave this alone.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Float && (Default = 0.2000)\\
\end{tabular}}\\
\texttt{-min <Float>} && \parbox[t]{3.5in}{Minimum Step Length (0.0001).  Increase this to make the convergence criterion less fussy.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& Float && (Default = 0.0001)\\
\end{tabular}}\\
\texttt{-BackgroundFillValue <FloatCode>} && \parbox[t]{3.5in}{Fill the background of image with specified value.  Enter number or use "BIGNEG" for a large negative number that compresses well with gzip, or "NaN" for the not a number representation.
\par
\begin{tabular}{p{0.5in}rp{0.25in}l}
& FloatCode && (Default = 0.0)\\
\end{tabular}}\\
\texttt{-fm <Filename>} && \parbox[t]{3.5in}{The fixed mask for registration.}\\
\texttt{-mm <Filename>} && \parbox[t]{3.5in}{The moving mask for registration.}\\
\texttt{-median <Integer>} && \parbox[t]{3.5in}{The radius for the optional MedianImageFilter preprocessing.}\\
\end{tabular}

\chapter{Specific tests} \label{sec:regression}
These tests are run from within \brainstwoprog{}.

\emph{These examples are out-of-date and need to be replaced.}

\section{Reproducing a specific shift}
\begin{verbatim}
MattesMutualRegistration \
-f B2-downsampled/TEST/20_ACPC/harris/10828302_20_shifted_T1.hdr \
-m B2-downsampled/TEST/20_ACPC/harris/10828302_20x20y30z_unshifted_T1.hdr \
-o B2-downsampled/TEST/20_ACPC/harris/MattesTestShift.xfrm \
-r B2-downsampled/TEST/20_ACPC/harris/MattesTestShift.hdr


b2 load image B2-downsampled/TEST/20_ACPC/harris/10828302_20x20y30z_unshifted_T1.hdr
b2 load image B2-downsampled/TEST/20_ACPC/harris/10828302_20_shifted_T1.hdr
b2 load image B2-downsampled/TEST/20_ACPC/harris/MattesTestShift.hdr
set i [b2 load image B2-downsampled/TEST/20_ACPC/harris/10828302_20x20y30z_unshifted_T1.hdr]
set tx [b2 load transform B2-downsampled/TEST/20_ACPC/harris/MattesTestShift.xfrm]
b2 set transform $tx image $i
\end{verbatim}

\section{Reproducing a specific rotation}
\begin{verbatim}
MattesMutualRegistration \
-f B2-downsampled/TEST/20_ACPC/harris/10828302_20_twisted_T1.hdr \
-m B2-downsampled/TEST/20_ACPC/harris/10828302_20x20y30z_unshifted_T1.hdr \
-o B2-downsampled/TEST/20_ACPC/harris/MattesTestTwist.xfrm \
-r B2-downsampled/TEST/20_ACPC/harris/MattesTestTwist.hdr


b2 load image B2-downsampled/TEST/20_ACPC/harris/10828302_20x20y30z_unshifted_T1.hdr
b2 load image B2-downsampled/TEST/20_ACPC/harris/10828302_20_twisted_T1.hdr
b2 load image B2-downsampled/TEST/20_ACPC/harris/MattesTestTwist.hdr
set i [b2 load image B2-downsampled/TEST/20_ACPC/harris/10828302_20x20y30z_unshifted_T1.hdr]
set tx [b2 load transform B2-downsampled/TEST/20_ACPC/harris/MattesTestTwist.xfrm]
b2 set transform $tx image $i
\end{verbatim}

\section{Reproducing a specific rotation and shift}
\begin{verbatim}
MattesMutualRegistration \
-f B2-downsampled/TEST/20_ACPC/harris/DistinctRotations_20_T1.hdr \
-m B2-downsampled/TEST/20_ACPC/harris/DistinctRotations_20_T1.hdr \
-io 1 \
-r B2-downsampled/TEST/20_ACPC/harris/MattesTest_Distinct.hdr

MattesMutualRegistration \
-f B2-downsampled/TEST/20_ACPC/harris/DistinctRotations_20_T1.hdr \
-m B2-downsampled/TEST/20_ACPC/10828302_20_T1.hdr \
-o B2-downsampled/TEST/20_ACPC/harris/MattesTest_Kahuna.xfrm \
-r B2-downsampled/TEST/20_ACPC/harris/MattesTest_Kahuna.hdr

MattesMutualRegistration \
-f B2-downsampled/TEST/20_ACPC/harris/DistinctRotations_20_T1.hdr \
-m B2-downsampled/TEST/20_ACPC/10828302_20_T1.hdr \
-i B2-downsampled/TEST/20_ACPC/harris/MattesTest_Kahuna.xfrm \
-o B2-downsampled/TEST/20_ACPC/harris/MattesTest_Redundant.xfrm \
-r B2-downsampled/TEST/20_ACPC/harris/MattesTest_Redundant.hdr


b2 load image B2-downsampled/TEST/20_ACPC/harris/MattesTest_Distinct.hdr
b2 load image B2-downsampled/TEST/20_ACPC/harris/MattesTest_Kahuna.hdr
set i [b2 load image B2-downsampled/TEST/20_ACPC/10828302_20_T1.hdr]
set tx [b2 load transform B2-downsampled/TEST/20_ACPC/harris/MattesTest_Kahuna.xfrm]
b2 set transform $tx image $i
set i [b2 load image B2-downsampled/TEST/20_ACPC/10828302_20_T1.hdr]
set tx [b2 load transform B2-downsampled/TEST/20_ACPC/harris/MattesTest_Redundant.xfrm]
b2 set transform $tx image $i
\end{verbatim}

\bibliographystyle{authordate1}
\bibliography{MutualRegistrationSoftware.bib}
\printindex
\end{document}
